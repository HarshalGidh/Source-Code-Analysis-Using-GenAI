{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\anaconda3\\envs\\llmapp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#To import Github repository \n",
    "from git import Repo \n",
    "from langchain.text_splitter import Language #To understand the code base Languade\n",
    "from langchain.document_loaders.generic import GenericLoader # To load the code base\n",
    "from langchain.document_loaders.parsers import LanguageParser  # Parses the Code Language from the Github repository\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone Github repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Dell\\\\Desktop\\\\Gen AI\\\\Source-Code-Analysis-Using-GenAI\\\\research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<git.repo.base.Repo 'c:\\\\Users\\\\Dell\\\\Desktop\\\\Gen AI\\\\Source-Code-Analysis-Using-GenAI\\\\research\\\\test_repo\\\\.git'>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "# Clone repository in test directory :\n",
    "Repo.clone_from(\"https://github.com/HarshalGidh/Student-Performance-Prediction\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cloned repository\n",
    "repo_path = \"test_repo/\"\n",
    "# Loads the Python Repository\n",
    "loader = GenericLoader.from_filesystem(repo_path + 'src' ,\n",
    "                                        glob = \"**/*\",\n",
    "                                       suffixes=[\".py\"],\n",
    "                                       parser = LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='import sys\\nfrom src.logger import logging\\n\\ndef error_message_detail(error,error_detail:sys):\\n    _,_,exc_tb = error_detail.exc_info()\\n    file_name=exc_tb.tb_frame.f_code.co_filename\\n    error_message=\"Error Occured in python script name [{0}] line number [{1}] error message [{2}] \".format(\\n        file_name,exc_tb.tb_lineno,str(error)\\n    )\\n    return error_message\\n\\nclass CustomException(Exception):\\n    def __init__(self,error_message,error_detail:sys):\\n        super().__init__(error_message)\\n        self.error_message=error_message_detail(error_message,error_detail=error_detail)\\n\\n    def __str__(self):\\n        return self.error_message \\n    \\n', metadata={'source': 'test_repo\\\\src\\\\exception.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import logging\\nimport os\\nfrom datetime import datetime\\n\\nLOG_FILE = f\"{datetime.now().strftime(\\'%m_%d_%Y_%H_%M_%S\\')}.log\"\\nlogs_path= os.path.join(os.getcwd(),\"logs\",LOG_FILE)\\nos.makedirs(logs_path,exist_ok=True)\\n\\nLOG_FILE_PATH = os.path.join(logs_path,LOG_FILE)\\n\\nlogging.basicConfig(\\n    filename=LOG_FILE_PATH,\\n    format=\"[ %(asctime)s] %(lineno)d %(name)s - %(levelname)s - %(message)s\",\\n    level=logging.INFO,\\n\\n)\\n', metadata={'source': 'test_repo\\\\src\\\\logger.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport sys\\n\\nimport numpy as np \\nimport pandas as pd\\nimport dill\\nimport pickle\\nfrom sklearn.metrics import r2_score\\nfrom sklearn.model_selection import GridSearchCV\\n\\nfrom src.exception import CustomException\\n\\ndef save_object(file_path, obj):\\n    try:\\n        dir_path = os.path.dirname(file_path)\\n\\n        os.makedirs(dir_path, exist_ok=True)\\n\\n        with open(file_path, \"wb\") as file_obj:\\n            pickle.dump(obj, file_obj)\\n\\n    except Exception as e:\\n        raise CustomException(e, sys)\\n    \\ndef evaluate_models(X_train, y_train,X_test,y_test,models,param):\\n    try:\\n        report = {}\\n\\n        for i in range(len(list(models))):\\n            model = list(models.values())[i]\\n            para=param[list(models.keys())[i]]\\n\\n            gs = GridSearchCV(model,para,cv=3)\\n            gs.fit(X_train,y_train)\\n\\n            model.set_params(**gs.best_params_)\\n            model.fit(X_train,y_train)\\n\\n            #model.fit(X_train, y_train)  # Train model\\n\\n            y_train_pred = model.predict(X_train)\\n\\n            y_test_pred = model.predict(X_test)\\n\\n            train_model_score = r2_score(y_train, y_train_pred)\\n\\n            test_model_score = r2_score(y_test, y_test_pred)\\n\\n            report[list(models.keys())[i]] = test_model_score\\n\\n        return report\\n\\n    except Exception as e:\\n        raise CustomException(e, sys)\\n    \\ndef load_object(file_path):\\n    try:\\n        with open(file_path,\"rb\") as file_obj:\\n            return dill.load(file_obj)\\n    except Exception as e:\\n        raise CustomException(e,sys)', metadata={'source': 'test_repo\\\\src\\\\utils.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport sys\\nfrom src.exception import CustomException\\nfrom src.logger import logging\\nimport pandas as pd\\n\\nfrom sklearn.model_selection import train_test_split\\nfrom dataclasses import dataclass\\nfrom src.components.data_transformation import DataTransformation\\nfrom src.components.data_transformation import DataTransformationConfig\\n\\nfrom src.components.model_trainer import ModelTrainerConfig\\nfrom src.components.model_trainer import ModelTrainer\\n\\n@dataclass\\nclass DataIngestionConfig:\\n    train_data_path : str = os.path.join(\\'artifacts\\',\"train.csv\")\\n    test_data_path : str = os.path.join(\\'artifacts\\',\"test.csv\")\\n    raw_data_path : str = os.path.join(\\'artifacts\\',\"data.csv\")\\n\\nclass DataIngestion:\\n    def __init__(self):\\n        self.ingestion_config = DataIngestionConfig()\\n    def initiate_data_ingestion(self):\\n        logging.info(\"Entered the data ingestion method or component \")\\n        try :\\n            df = pd.read_csv(\\'Notebook\\\\data\\\\stud.csv\\')\\n            logging.info(\\'Read the Dataset as a DataFrame \\')\\n\\n            os.makedirs(os.path.dirname(self.ingestion_config.train_data_path), exist_ok=True)\\n            df.to_csv(self.ingestion_config.raw_data_path,index=False,header=True)\\n            logging.info(\"Train test Split Initiated \")\\n\\n            train_set,test_set = train_test_split(df,test_size=0.2,random_state=42)\\n\\n            train_set.to_csv(self.ingestion_config.train_data_path,index=False,header=True)\\n\\n            test_set.to_csv(self.ingestion_config.test_data_path,index=False,header=True)\\n\\n            logging.info(\\'Ingestion of Data is Completed \\')\\n            return (\\n                self.ingestion_config.train_data_path,\\n                self.ingestion_config.test_data_path\\n\\n            )\\n        except Exception as e:\\n            raise CustomException(e,sys)\\n        \\n\\nif __name__==\"__main__\":\\n    obj = DataIngestion()\\n    train_data,test_data =obj.initiate_data_ingestion()\\n\\n    data_transformation = DataTransformation()\\n    #taking train arr and test arr (not importing model again)\\n    train_arr,test_arr,_ =data_transformation.initiate_data_transformation(train_data,test_data)\\n    model_Trainer = ModelTrainer()\\n    print(model_Trainer.initiate_model_trainer(train_arr,test_arr))\\n', metadata={'source': 'test_repo\\\\src\\\\components\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import sys\\nfrom dataclasses import dataclass\\n\\nimport numpy as np \\nimport pandas as pd\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import OneHotEncoder,StandardScaler\\n\\nfrom src.exception import CustomException\\nfrom src.logger import logging\\nimport os\\n\\nfrom src.utils import save_object\\n\\n@dataclass\\nclass DataTransformationConfig:\\n    preprocessor_obj_file_path=os.path.join(\\'artifacts\\',\"proprocessor.pkl\")\\n\\nclass DataTransformation:\\n    def __init__(self):\\n        self.data_transformation_config=DataTransformationConfig()\\n\\n    def get_data_transformer_object(self):\\n        \\'\\'\\'\\n        This function si responsible for data trnasformation\\n        \\n        \\'\\'\\'\\n        try:\\n            numerical_columns = [\"writing_score\", \"reading_score\"]\\n            categorical_columns = [\\n                \"gender\",\\n                \"race_ethnicity\",\\n                \"parental_level_of_education\",\\n                \"lunch\",\\n                \"test_preparation_course\",\\n            ]\\n\\n            num_pipeline= Pipeline(\\n                steps=[\\n                (\"imputer\",SimpleImputer(strategy=\"median\")),\\n                (\"scaler\",StandardScaler())\\n\\n                ]\\n            )\\n\\n            cat_pipeline=Pipeline(\\n\\n                steps=[\\n                (\"imputer\",SimpleImputer(strategy=\"most_frequent\")),\\n                (\"one_hot_encoder\",OneHotEncoder()),\\n                (\"scaler\",StandardScaler(with_mean=False))\\n                ]\\n\\n            )\\n\\n            logging.info(f\"Categorical columns: {categorical_columns}\")\\n            logging.info(f\"Numerical columns: {numerical_columns}\")\\n\\n            preprocessor=ColumnTransformer(\\n                [\\n                (\"num_pipeline\",num_pipeline,numerical_columns),\\n                (\"cat_pipelines\",cat_pipeline,categorical_columns)\\n\\n                ]\\n\\n\\n            )\\n\\n            return preprocessor\\n        \\n        except Exception as e:\\n            raise CustomException(e,sys)\\n        \\n    def initiate_data_transformation(self,train_path,test_path):\\n\\n        try:\\n            train_df=pd.read_csv(train_path)\\n            test_df=pd.read_csv(test_path)\\n\\n            logging.info(\"Read train and test data completed\")\\n\\n            logging.info(\"Obtaining preprocessing object\")\\n\\n            preprocessing_obj=self.get_data_transformer_object()\\n\\n            target_column_name=\"math_score\"\\n            numerical_columns = [\"writing_score\", \"reading_score\"]\\n\\n            input_feature_train_df=train_df.drop(columns=[target_column_name],axis=1)\\n            target_feature_train_df=train_df[target_column_name]\\n\\n            input_feature_test_df=test_df.drop(columns=[target_column_name],axis=1)\\n            target_feature_test_df=test_df[target_column_name]\\n\\n            logging.info(\\n                f\"Applying preprocessing object on training dataframe and testing dataframe.\"\\n            )\\n\\n            input_feature_train_arr=preprocessing_obj.fit_transform(input_feature_train_df)\\n            input_feature_test_arr=preprocessing_obj.transform(input_feature_test_df)\\n\\n            train_arr = np.c_[\\n                input_feature_train_arr, np.array(target_feature_train_df)\\n            ]\\n            test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test_df)]\\n\\n            logging.info(f\"Saved preprocessing object.\")\\n\\n            save_object(\\n\\n                file_path=self.data_transformation_config.preprocessor_obj_file_path,\\n                obj=preprocessing_obj\\n\\n            )\\n\\n            return (\\n                train_arr,\\n                test_arr,\\n                self.data_transformation_config.preprocessor_obj_file_path,\\n            )\\n        except Exception as e:\\n            raise CustomException(e,sys)', metadata={'source': 'test_repo\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport sys\\nfrom dataclasses import dataclass\\n\\nfrom catboost import CatBoostRegressor\\nfrom sklearn.ensemble import (\\n    AdaBoostRegressor,\\n    GradientBoostingRegressor,\\n    RandomForestRegressor,\\n)\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.metrics import r2_score\\nfrom sklearn.neighbors import KNeighborsRegressor\\nfrom sklearn.tree import DecisionTreeRegressor\\nfrom xgboost import XGBRegressor\\n\\nfrom src.exception import CustomException\\nfrom src.logger import logging\\n\\nfrom src.utils import save_object,evaluate_models\\n\\n@dataclass\\nclass ModelTrainerConfig:\\n    trained_model_file_path=os.path.join(\"artifacts\",\"model.pkl\")\\n\\nclass ModelTrainer:\\n    def __init__(self):\\n        self.model_trainer_config=ModelTrainerConfig()\\n\\n\\n    def initiate_model_trainer(self,train_array,test_array):\\n        try:\\n            logging.info(\"Split training and test input data\")\\n            X_train,y_train,X_test,y_test=(\\n                train_array[:,:-1],\\n                train_array[:,-1],\\n                test_array[:,:-1],\\n                test_array[:,-1]\\n            )\\n            models = {\\n                \"Random Forest\": RandomForestRegressor(),\\n                \"Decision Tree\": DecisionTreeRegressor(),\\n                \"Gradient Boosting\": GradientBoostingRegressor(),\\n                \"Linear Regression\": LinearRegression(),\\n                \"XGBRegressor\": XGBRegressor(),\\n                \"CatBoosting Regressor\": CatBoostRegressor(verbose=False),\\n                \"AdaBoost Regressor\": AdaBoostRegressor(),\\n            }\\n            params={\\n                \"Decision Tree\": {\\n                    \\'criterion\\':[\\'squared_error\\', \\'friedman_mse\\', \\'absolute_error\\', \\'poisson\\'],\\n                    # \\'splitter\\':[\\'best\\',\\'random\\'],\\n                    # \\'max_features\\':[\\'sqrt\\',\\'log2\\'],\\n                },\\n                \"Random Forest\":{\\n                    # \\'criterion\\':[\\'squared_error\\', \\'friedman_mse\\', \\'absolute_error\\', \\'poisson\\'],\\n                 \\n                    # \\'max_features\\':[\\'sqrt\\',\\'log2\\',None],\\n                    \\'n_estimators\\': [8,16,32,64,128,256]\\n                },\\n                \"Gradient Boosting\":{\\n                    # \\'loss\\':[\\'squared_error\\', \\'huber\\', \\'absolute_error\\', \\'quantile\\'],\\n                    \\'learning_rate\\':[.1,.01,.05,.001],\\n                    \\'subsample\\':[0.6,0.7,0.75,0.8,0.85,0.9],\\n                    # \\'criterion\\':[\\'squared_error\\', \\'friedman_mse\\'],\\n                    # \\'max_features\\':[\\'auto\\',\\'sqrt\\',\\'log2\\'],\\n                    \\'n_estimators\\': [8,16,32,64,128,256]\\n                },\\n                \"Linear Regression\":{},\\n                \"XGBRegressor\":{\\n                    \\'learning_rate\\':[.1,.01,.05,.001],\\n                    \\'n_estimators\\': [8,16,32,64,128,256]\\n                },\\n                \"CatBoosting Regressor\":{\\n                    \\'depth\\': [6,8,10],\\n                    \\'learning_rate\\': [0.01, 0.05, 0.1],\\n                    \\'iterations\\': [30, 50, 100]\\n                },\\n                \"AdaBoost Regressor\":{\\n                    \\'learning_rate\\':[.1,.01,0.5,.001],\\n                    # \\'loss\\':[\\'linear\\',\\'square\\',\\'exponential\\'],\\n                    \\'n_estimators\\': [8,16,32,64,128,256]\\n                }\\n                \\n            }\\n\\n            model_report:dict=evaluate_models(X_train=X_train,y_train=y_train,X_test=X_test,y_test=y_test,\\n                                             models=models,param=params)\\n            \\n            ## To get best model score from dict\\n            best_model_score = max(sorted(model_report.values()))\\n\\n            ## To get best model name from dict\\n\\n            best_model_name = list(model_report.keys())[\\n                list(model_report.values()).index(best_model_score)\\n            ]\\n            best_model = models[best_model_name]\\n\\n            if best_model_score<0.6:\\n                raise CustomException(\"No best model found\")\\n            logging.info(f\"Best found model on both training and testing dataset\")\\n\\n            save_object(\\n                file_path=self.model_trainer_config.trained_model_file_path,\\n                obj=best_model\\n            )\\n\\n            predicted=best_model.predict(X_test)\\n\\n            r2_square = r2_score(y_test, predicted)\\n            return r2_square\\n            \\n\\n\\n\\n            \\n        except Exception as e:\\n            raise CustomException(e,sys)', metadata={'source': 'test_repo\\\\src\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\components\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import sys\\nimport pandas as pd\\nfrom src.exception import CustomException\\nfrom src.utils import load_object\\nimport os\\n\\nclass PredictPipeline:\\n    def __init__(self):\\n        pass\\n    \\n    #load Model and Preprocessor :\\n    def predict(self,features):\\n        try:\\n            model_path=os.path.join(\"artifacts\",\"model.pkl\")\\n            preprocessor_path=os.path.join(\\'artifacts\\',\\'preprocessor.pkl\\')\\n            print(\"Before Loading\")\\n            model=load_object(file_path=model_path)\\n            preprocessor=load_object(file_path=preprocessor_path)\\n            print(\"After Loading\")\\n            data_scaled=preprocessor.transform(features)\\n            preds=model.predict(data_scaled)\\n            return preds\\n        \\n        except Exception as e:\\n            raise CustomException(e,sys)\\n\\n# Creating a Custom Data Class which will map all the inputs from the webpage to the backend for Prediction\\nclass CustomData:\\n    def __init__(  self,\\n        gender: str,\\n        race_ethnicity: str,\\n        parental_level_of_education,\\n        lunch: str,\\n        test_preparation_course: str,\\n        reading_score: int,\\n        writing_score: int):\\n\\n        self.gender = gender\\n\\n        self.race_ethnicity = race_ethnicity\\n\\n        self.parental_level_of_education = parental_level_of_education\\n\\n        self.lunch = lunch\\n\\n        self.test_preparation_course = test_preparation_course\\n\\n        self.reading_score = reading_score\\n\\n        self.writing_score = writing_score\\n\\n    #Converting and Returning all inputs as a DataFrame :\\n    def get_data_as_data_frame(self):\\n        try:\\n            custom_data_input_dict = {\\n                \"gender\": [self.gender],\\n                \"race_ethnicity\": [self.race_ethnicity],\\n                \"parental_level_of_education\": [self.parental_level_of_education],\\n                \"lunch\": [self.lunch],\\n                \"test_preparation_course\": [self.test_preparation_course],\\n                \"reading_score\": [self.reading_score],\\n                \"writing_score\": [self.writing_score],\\n            }\\n\\n            return pd.DataFrame(custom_data_input_dict)\\n\\n        except Exception as e:\\n            raise CustomException(e, sys)', metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\predict_pipeline.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\train_pipeline.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\__init__.py', 'language': <Language.PYTHON: 'python'>})]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunkings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Context Aware Splitting : \n",
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(language = Language.PYTHON,\n",
    "                                                             chunk_size = 2000,\n",
    "                                                             chunk_overlap = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] =  \"***************************\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = \"***************************\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",disallowed_special=(),google_api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge base (vector DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(texts #data\n",
    "                                 , embedding=embeddings,# embedding model\n",
    "                                   persist_directory='./data') #directory to store data\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creatring Retriever object\n",
    "retriever = vectordb.as_retriever() \n",
    "#  If the length is greater than zero, it means that the retriever is functioning well.\n",
    "print(len(retriever.get_relevant_documents(\"data ingestion\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatOpenAI(model_name=\"gpt-4\")\n",
    "# llm = ChatOpenAI()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
    "                 temperature=0.7, top_p=0.85,google_api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm, memory_key = \"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":3}), memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is DataIngestion class?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] template=\"You are an assistant for question-answering tasks.\\nUse the following context to answer the question.\\nIf you don't know the answer, just say that you don't know.\\nUse five sentences maximum and keep the answer concise.\\n\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"\n"
     ]
    }
   ],
   "source": [
    "# Prompt template to query Gemini\n",
    "llm_prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use five sentences maximum and keep the answer concise.\\n\n",
    "Question: {question} \\nContext: {context} \\nAnswer:\"\"\"\n",
    "\n",
    "llm_prompt = PromptTemplate.from_template(llm_prompt_template)\n",
    "\n",
    "print(llm_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine data from documents to readable string format.\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | llm_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The DataIngestion class is responsible for reading and splitting the raw data into training and testing sets. It also saves the training and testing sets to the specified file paths.\\n\\nThe class has an __init__ method that initializes the ingestion configuration and an initiate_data_ingestion method that reads the raw data, splits it into training and testing sets, and saves the sets to the specified file paths.\\n\\nThe initiate_data_ingestion method first reads the raw data into a DataFrame. It then creates the directories for the training and testing data if they do not already exist. The DataFrame is then saved to the raw data file path.\\n\\nThe DataFrame is then split into training and testing sets using the train_test_split function from the sklearn.model_selection module. The training set is saved to the training data file path and the testing set is saved to the testing data file path.\\n\\nThe initiate_data_ingestion method returns the file paths of the training and testing data sets.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
